{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from rasa.nlu.model import Interpreter\n",
    "from rasa.cli.utils import get_validated_path\n",
    "from rasa.constants import DEFAULT_MODELS_PATH\n",
    "from rasa.model import get_model, get_model_subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/d/dev/sunmi/git/chatbot_nlu/cs/tests\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = Path('.').absolute().parents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\wegam\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.751 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server config:\n",
      "                        client\t=\tffcbb6e1-5370-446e-9500-cda4baae0209\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpxU77SW/socket', 'ipc://tmprAoqej/socket', 'ipc://tmpDlRKzF/socket', 'ipc://tmprjr7U1/socket', 'ipc://tmpvFewgo/socket', 'ipc://tmp761XBK/socket', 'ipc://tmpZLXrX6/socket', 'ipc://tmpkznYit/socket']\n",
      "                worker -> sink\t=\tipc://tmpRvQjdr/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpM9bSxA/socket        \n",
      "           server_current_time\t=\t2019-08-29 13:48:40.603695    \n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t50                            \n",
      "                     model_dir\t=\tchinese_L-12_H-768_A-12       \n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t5555                          \n",
      "                      port_out\t=\t5556                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.9.6                         \n",
      "                 pyzmq_version\t=\t18.0.2                        \n",
      "                   zmq_version\t=\t4.3.1                         \n",
      "             server_start_time\t=\t2019-08-29 11:04:33.799497    \n",
      "server config:\n",
      "                        client\t=\tf5dc2b1b-b8da-4d77-8464-1d1916c4ca03\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpxU77SW/socket', 'ipc://tmprAoqej/socket', 'ipc://tmpDlRKzF/socket', 'ipc://tmprjr7U1/socket', 'ipc://tmpvFewgo/socket', 'ipc://tmp761XBK/socket', 'ipc://tmpZLXrX6/socket', 'ipc://tmpkznYit/socket']\n",
      "                worker -> sink\t=\tipc://tmpRvQjdr/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpM9bSxA/socket        \n",
      "           server_current_time\t=\t2019-08-29 13:48:40.783586    \n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t50                            \n",
      "                     model_dir\t=\tchinese_L-12_H-768_A-12       \n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t5555                          \n",
      "                      port_out\t=\t5556                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.9.6                         \n",
      "                 pyzmq_version\t=\t18.0.2                        \n",
      "                   zmq_version\t=\t4.3.1                         \n",
      "             server_start_time\t=\t2019-08-29 11:04:33.799497    \n",
      "server config:\n",
      "                        client\t=\tcde8c33e-9692-4fc8-8e8e-1a5456cce527\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpxU77SW/socket', 'ipc://tmprAoqej/socket', 'ipc://tmpDlRKzF/socket', 'ipc://tmprjr7U1/socket', 'ipc://tmpvFewgo/socket', 'ipc://tmp761XBK/socket', 'ipc://tmpZLXrX6/socket', 'ipc://tmpkznYit/socket']\n",
      "                worker -> sink\t=\tipc://tmpRvQjdr/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpM9bSxA/socket        \n",
      "           server_current_time\t=\t2019-08-29 13:48:40.982771    \n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t50                            \n",
      "                     model_dir\t=\tchinese_L-12_H-768_A-12       \n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t5555                          \n",
      "                      port_out\t=\t5556                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                server_version\t=\t1.9.6                         \n",
      "                 pyzmq_version\t=\t18.0.2                        \n",
      "                   zmq_version\t=\t4.3.1                         \n",
      "             server_start_time\t=\t2019-08-29 11:04:33.799497    \n",
      "server config:\n",
      "                        client\t=\tf09b507e-f199-4c8d-89d6-86219818aa78\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpxU77SW/socket', 'ipc://tmprAoqej/socket', 'ipc://tmpDlRKzF/socket', 'ipc://tmprjr7U1/socket', 'ipc://tmpvFewgo/socket', 'ipc://tmp761XBK/socket', 'ipc://tmpZLXrX6/socket', 'ipc://tmpkznYit/socket']\n",
      "                worker -> sink\t=\tipc://tmpRvQjdr/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpM9bSxA/socket        \n",
      "           server_current_time\t=\t2019-08-29 13:48:41.181073    \n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t50                            \n",
      "                     model_dir\t=\tchinese_L-12_H-768_A-12       \n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t5555                          \n",
      "                      port_out\t=\t5556                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.9.6                         \n",
      "                 pyzmq_version\t=\t18.0.2                        \n",
      "                   zmq_version\t=\t4.3.1                         \n",
      "             server_start_time\t=\t2019-08-29 11:04:33.799497    \n",
      "server config:\n",
      "                        client\t=\t605099e7-0c23-4f31-9e56-552e5f9ebdec\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpxU77SW/socket', 'ipc://tmprAoqej/socket', 'ipc://tmpDlRKzF/socket', 'ipc://tmprjr7U1/socket', 'ipc://tmpvFewgo/socket', 'ipc://tmp761XBK/socket', 'ipc://tmpZLXrX6/socket', 'ipc://tmpkznYit/socket']\n",
      "                worker -> sink\t=\tipc://tmpRvQjdr/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpM9bSxA/socket        \n",
      "           server_current_time\t=\t2019-08-29 13:48:41.349788    \n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t50                            \n",
      "                     model_dir\t=\tchinese_L-12_H-768_A-12       \n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t5555                          \n",
      "                      port_out\t=\t5556                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.9.6                         \n",
      "                 pyzmq_version\t=\t18.0.2                        \n",
      "                   zmq_version\t=\t4.3.1                         \n",
      "             server_start_time\t=\t2019-08-29 11:04:33.799497    \n",
      "server config:\n",
      "                        client\t=\tbafabd77-c00e-4b86-83a8-12998e216af5\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpxU77SW/socket', 'ipc://tmprAoqej/socket', 'ipc://tmpDlRKzF/socket', 'ipc://tmprjr7U1/socket', 'ipc://tmpvFewgo/socket', 'ipc://tmp761XBK/socket', 'ipc://tmpZLXrX6/socket', 'ipc://tmpkznYit/socket']\n",
      "                worker -> sink\t=\tipc://tmpRvQjdr/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpM9bSxA/socket        \n",
      "           server_current_time\t=\t2019-08-29 13:48:41.521795    \n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t50                            \n",
      "                     model_dir\t=\tchinese_L-12_H-768_A-12       \n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t5555                          \n",
      "                      port_out\t=\t5556                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.9.6                         \n",
      "                 pyzmq_version\t=\t18.0.2                        \n",
      "                   zmq_version\t=\t4.3.1                         \n",
      "             server_start_time\t=\t2019-08-29 11:04:33.799497    \n",
      "server config:\n",
      "                        client\t=\t4454bdb2-55bb-4ec0-9136-2b3df3da8bbd\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpxU77SW/socket', 'ipc://tmprAoqej/socket', 'ipc://tmpDlRKzF/socket', 'ipc://tmprjr7U1/socket', 'ipc://tmpvFewgo/socket', 'ipc://tmp761XBK/socket', 'ipc://tmpZLXrX6/socket', 'ipc://tmpkznYit/socket']\n",
      "                worker -> sink\t=\tipc://tmpRvQjdr/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpM9bSxA/socket        \n",
      "           server_current_time\t=\t2019-08-29 13:48:41.732714    \n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t50                            \n",
      "                     model_dir\t=\tchinese_L-12_H-768_A-12       \n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t5555                          \n",
      "                      port_out\t=\t5556                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.9.6                         \n",
      "                 pyzmq_version\t=\t18.0.2                        \n",
      "                   zmq_version\t=\t4.3.1                         \n",
      "             server_start_time\t=\t2019-08-29 11:04:33.799497    \n",
      "server config:\n",
      "                        client\t=\t21f23dfa-0a24-4a19-a330-0f96beb9f77a\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpxU77SW/socket', 'ipc://tmprAoqej/socket', 'ipc://tmpDlRKzF/socket', 'ipc://tmprjr7U1/socket', 'ipc://tmpvFewgo/socket', 'ipc://tmp761XBK/socket', 'ipc://tmpZLXrX6/socket', 'ipc://tmpkznYit/socket']\n",
      "                worker -> sink\t=\tipc://tmpRvQjdr/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpM9bSxA/socket        \n",
      "           server_current_time\t=\t2019-08-29 13:48:41.917323    \n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t50                            \n",
      "                     model_dir\t=\tchinese_L-12_H-768_A-12       \n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t5555                          \n",
      "                      port_out\t=\t5556                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.9.6                         \n",
      "                 pyzmq_version\t=\t18.0.2                        \n",
      "                   zmq_version\t=\t4.3.1                         \n",
      "             server_start_time\t=\t2019-08-29 11:04:33.799497    \n",
      "server config:\n",
      "                        client\t=\tb65a2427-db6e-4fd9-b5a2-2f5be05fff1a\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpxU77SW/socket', 'ipc://tmprAoqej/socket', 'ipc://tmpDlRKzF/socket', 'ipc://tmprjr7U1/socket', 'ipc://tmpvFewgo/socket', 'ipc://tmp761XBK/socket', 'ipc://tmpZLXrX6/socket', 'ipc://tmpkznYit/socket']\n",
      "                worker -> sink\t=\tipc://tmpRvQjdr/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpM9bSxA/socket        \n",
      "           server_current_time\t=\t2019-08-29 13:48:42.089280    \n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t50                            \n",
      "                     model_dir\t=\tchinese_L-12_H-768_A-12       \n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t5555                          \n",
      "                      port_out\t=\t5556                          \n",
      "                 prefetch_size\t=\t10                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.9.6                         \n",
      "                 pyzmq_version\t=\t18.0.2                        \n",
      "                   zmq_version\t=\t4.3.1                         \n",
      "             server_start_time\t=\t2019-08-29 11:04:33.799497    \n",
      "server config:\n",
      "                        client\t=\tb3142b92-81cb-4fa7-b986-815211bbb384\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpxU77SW/socket', 'ipc://tmprAoqej/socket', 'ipc://tmpDlRKzF/socket', 'ipc://tmprjr7U1/socket', 'ipc://tmpvFewgo/socket', 'ipc://tmp761XBK/socket', 'ipc://tmpZLXrX6/socket', 'ipc://tmpkznYit/socket']\n",
      "                worker -> sink\t=\tipc://tmpRvQjdr/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpM9bSxA/socket        \n",
      "           server_current_time\t=\t2019-08-29 13:48:42.264649    \n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t50                            \n",
      "                     model_dir\t=\tchinese_L-12_H-768_A-12       \n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t5555                          \n",
      "                      port_out\t=\t5556                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.9.6                         \n",
      "                 pyzmq_version\t=\t18.0.2                        \n",
      "                   zmq_version\t=\t4.3.1                         \n",
      "             server_start_time\t=\t2019-08-29 11:04:33.799497    \n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\wegam\\AppData\\Local\\Temp\\tmpv93jwy51\\nlu\\component_4_EmbeddingBertIntentClassifier.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = str(parent_folder / \"models\")\n",
    "model = get_validated_path(model, \"model\", DEFAULT_MODELS_PATH)\n",
    "model_path = get_model(model)\n",
    "_, nlu_model = get_model_subdirectories(model_path)\n",
    "interpreter = Interpreter.load(nlu_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Raw Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('chats.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of services: 8\n",
      "Number of Groups: 177\n",
      "Number of Users: 1128\n",
      "Number of Users: 25109\n"
     ]
    }
   ],
   "source": [
    "num_services = len(df.from_user_name.apply(find_kf).unique()) - 1\n",
    "num_groups = len(df.chat_group_wxid.unique())\n",
    "num_customers = len(df.from_user_wxid.unique()) - services\n",
    "num_chats = len(df)\n",
    "\n",
    "print('Number of services: {0}'.format(num_services))\n",
    "print('Number of Groups: {0}'.format(num_groups))\n",
    "print('Number of Users: {0}'.format(num_customers))\n",
    "print('Number of Users: {0}'.format(num_chats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter out Useful Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\rasa\\lib\\site-packages\\bert_serving\\client\\__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=50\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "parsed_result = df.chat_text.apply(lambda x: interpreter.parse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_question = [True if r['intent']['name'] == 'question' else False for r in parsed_result]\n",
    "df['is_question'] = is_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar as question: 15241\n"
     ]
    }
   ],
   "source": [
    "print(\"Similar as question: {0}\".format(np.sum(is_question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
